--
-- This file has been pre-processed with DynASM.
-- http://luajit.org/dynasm.html
-- DynASM version 1.4.0_luamode, DynASM x64 version 1.4.0_luamode
-- DO NOT EDIT! The original file is in "lib/ipsec/aes_gcm_avx.dasl".
--

-- Use of this source code is governed by the Apache 2.0 license; see COPYING.

-- Selected AES GCM routines, based heavily on the Intel IPsec code from:
-- https://github.com/lukego/intel-ipsec/blob/master/code/avx2/gcm_avx_gen4.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/gcm_defines.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/aes_keyexp_128.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/aes_keyexp_256.asm

local dasm = require("dasm")
local ffi = require("ffi")

ffi.cdef[[
typedef struct gcm_data
{
  uint8_t expanded_keys[16*15];
  uint8_t shifted_hkey_1[16];
  uint8_t shifted_hkey_2[16];
  uint8_t shifted_hkey_3[16];
  uint8_t shifted_hkey_4[16];
  uint8_t shifted_hkey_5[16];
  uint8_t shifted_hkey_6[16];
  uint8_t shifted_hkey_7[16];
  uint8_t shifted_hkey_8[16];
  uint8_t shifted_hkey_1_k[16];
  uint8_t shifted_hkey_2_k[16];
  uint8_t shifted_hkey_3_k[16];
  uint8_t shifted_hkey_4_k[16];
  uint8_t shifted_hkey_5_k[16];
  uint8_t shifted_hkey_6_k[16];
  uint8_t shifted_hkey_7_k[16];
  uint8_t shifted_hkey_8_k[16];
} gcm_data;
]]

--|.arch x64
if dasm._VERSION ~= 10400 then
  error("Version mismatch between DynASM and included encoding engine")
end
--|.actionlist actions
local actions = ffi.new('const uint8_t[1928]', {
  197,252,249,111,5,240,132,244,10,196,227,121,240,160,68,192,240,133,240,37,
  1,196,225,121,240,160,115,252,248,240,45,8,196,225,121,240,160,252,239,192,
  240,133,240,45,196,227,121,240,160,68,192,240,133,240,37,0,196,225,121,240,
  160,115,216,240,45,4,196,227,121,240,160,68,192,240,133,240,37,16,255,196,
  225,121,240,160,115,252,248,240,45,4,196,225,121,240,160,252,239,192,240,
  133,240,45,196,225,121,240,160,252,239,192,240,133,240,45,255,196,227,121,
  240,160,68,192,240,133,240,37,17,196,227,121,240,160,68,192,240,133,240,37,
  0,196,227,121,240,160,68,192,240,133,240,37,1,196,227,121,240,160,68,192,
  240,133,240,37,16,196,225,121,240,160,252,239,192,240,133,240,45,255,196,
  225,121,240,160,115,216,240,45,8,196,225,121,240,160,115,252,248,240,45,8,
  196,225,121,240,160,252,239,192,240,133,240,45,196,225,121,240,160,252,239,
  192,240,133,240,45,255,197,252,249,240,160,252,254,5,240,132,244,11,255,197,
  252,249,240,160,252,254,5,240,132,244,12,255,196,225,121,111,192,240,133,
  240,45,255,196,226,121,240,160,0,5,240,133,244,13,255,197,252,249,111,135,
  253,240,132,233,255,196,226,121,240,160,220,192,240,133,240,37,255,196,226,
  121,240,160,221,192,240,133,240,37,255,196,161,122,111,132,253,240,133,26,
  233,196,225,121,240,160,252,239,192,240,133,240,45,196,161,122,127,132,253,
  240,133,30,233,255,73,129,195,239,255,196,193,122,111,1,240,133,196,226,121,
  240,160,0,5,240,133,244,13,255,77,49,219,76,137,192,197,252,250,111,0,240,
  132,196,226,121,240,160,0,5,240,133,244,13,255,197,252,250,111,135,253,240,
  132,233,255,197,121,127,4,36,196,193,121,111,192,240,133,73,129,252,253,128,
  0,0,0,15,140,244,255,255,197,252,241,252,239,12,36,248,9,255,196,227,121,
  240,160,68,192,240,133,240,37,235,255,65,128,199,8,197,252,249,111,193,240,
  132,255,197,252,249,127,132,253,240,132,36,233,255,197,252,249,111,132,253,
  240,132,36,233,255,196,161,121,240,160,252,239,132,253,240,133,26,233,255,
  196,226,121,240,160,221,192,240,133,240,37,196,161,122,127,132,253,240,133,
  30,233,255,196,226,121,240,160,221,192,240,133,240,37,196,161,122,111,132,
  253,240,133,26,233,196,161,122,127,132,253,240,133,30,233,255,196,225,121,
  240,160,115,252,248,240,45,8,196,225,121,240,160,115,216,240,45,8,196,225,
  121,240,160,252,239,192,240,133,240,45,196,225,121,240,160,252,239,192,240,
  133,240,45,255,196,225,113,252,239,200,240,45,73,129,195,128,0,0,0,73,129,
  252,237,128,0,0,0,255,197,252,249,111,135,253,240,132,233,196,225,121,112,
  192,240,133,240,45,78,196,225,121,112,192,240,133,240,45,78,196,225,121,240,
  160,252,239,192,240,133,240,45,196,225,121,240,160,252,239,192,240,133,240,
  45,255,196,225,113,252,239,200,240,45,196,225,113,252,239,192,240,133,240,
  45,255,196,65,121,111,252,247,255,197,252,249,240,160,252,239,135,253,240,
  132,233,255,196,226,121,240,160,220,135,253,240,133,233,255,196,226,121,240,
  160,221,135,253,240,133,233,255,64,80,240,42,255,73,137,230,72,129,252,236,
  239,72,131,228,192,255,76,137,252,244,255,64,88,240,42,255,195,255,73,137,
  205,73,131,229,252,240,77,137,252,236,73,193,252,236,4,73,131,228,7,15,132,
  245,255,73,129,252,252,239,15,132,245,255,252,233,245,255,249,255,73,129,
  252,237,239,252,233,244,254,255,248,8,73,131,252,253,0,15,132,244,247,73,
  129,252,237,128,0,0,0,15,132,244,248,196,65,121,126,207,65,129,231,252,255,
  0,0,0,196,98,49,0,13,244,13,248,3,65,128,252,255,235,15,143,244,250,255,15,
  133,244,3,196,98,49,0,13,244,13,252,233,244,248,248,4,196,98,49,0,13,244,
  13,255,196,98,49,0,13,244,13,15,133,244,3,196,98,49,0,13,244,13,248,2,255,
  248,1,255,73,137,205,73,131,229,15,15,132,244,247,255,197,49,252,254,13,244,
  11,196,98,49,0,13,244,13,255,73,131,252,235,16,77,1,252,235,196,161,122,111,
  12,26,76,141,37,244,14,77,41,252,236,196,193,122,111,20,36,196,226,113,0,
  202,255,197,252,249,111,209,255,197,49,252,239,201,196,193,122,111,140,253,
  36,233,197,49,219,201,255,197,252,233,219,209,255,196,193,121,111,209,255,
  196,226,105,0,21,244,13,197,9,252,239,252,242,255,77,41,252,235,73,131,195,
  16,255,196,97,252,249,126,200,73,131,252,253,8,15,142,244,248,74,137,4,30,
  73,131,195,8,196,193,49,115,217,8,196,97,252,249,126,200,73,131,252,237,8,
  248,2,66,136,4,30,73,131,195,1,72,193,232,8,73,131,252,237,1,15,133,244,2,
  255,248,1,77,139,166,233,73,193,228,3,196,65,121,110,252,252,255,72,193,225,
  3,196,225,252,249,110,201,196,193,1,115,252,255,8,197,1,252,239,252,249,255,
  196,65,9,252,239,252,247,255,196,98,9,0,53,244,13,76,137,192,197,122,111,
  8,255,196,65,49,252,239,206,255,77,139,150,233,77,139,158,233,73,131,252,
  251,16,15,132,244,249,73,131,252,251,12,15,132,244,248,196,97,252,249,126,
  200,73,137,2,252,233,244,250,248,2,196,97,252,249,126,200,73,137,2,196,193,
  49,115,217,8,197,121,126,200,65,137,130,233,252,233,244,250,248,3,196,65,
  122,127,10,248,4,255,197,252,250,111,54,196,226,73,0,53,244,13,197,252,249,
  111,214,197,201,115,252,246,1,197,252,233,115,210,63,197,252,249,111,202,
  197,252,233,115,252,250,8,197,252,241,115,217,8,197,201,252,235,252,242,197,
  252,249,112,209,36,197,252,233,118,21,244,15,197,252,233,219,21,244,16,197,
  201,252,239,252,242,197,252,249,127,183,233,255,197,252,249,111,230,255,197,
  252,249,127,167,233,255,197,252,249,112,210,252,255,197,224,198,217,16,197,
  252,241,252,239,203,197,224,198,217,140,197,252,241,252,239,203,197,252,241,
  252,239,202,255,197,252,249,112,210,170,197,224,198,220,16,197,217,252,239,
  227,197,224,198,220,140,197,217,252,239,227,197,217,252,239,226,255,197,252,
  250,111,15,197,252,249,127,14,197,225,252,239,219,255,196,227,121,223,209,
  235,255,197,252,249,127,142,233,255,197,252,250,111,15,197,252,249,127,14,
  197,252,250,111,103,16,197,252,249,127,102,16,197,225,252,239,219,255,196,
  227,121,223,212,235,255,197,252,249,127,142,233,196,227,121,223,209,235,255,
  197,252,249,127,166,233,255,196,227,121,223,212,64,255,197,252,249,127,142,
  233,195,255,197,252,250,111,151,233,73,137,210,73,137,204,255,197,252,249,
  252,239,192,255,248,1,73,131,252,252,16,15,142,244,248,196,193,122,111,10,
  196,226,113,0,13,244,13,197,252,249,252,239,193,255,73,131,194,16,73,131,
  252,236,16,252,233,244,1,255,248,2,77,137,227,197,252,241,252,239,201,248,
  1,196,193,121,110,26,197,225,115,252,251,12,197,252,241,115,217,4,197,252,
  241,252,239,203,73,131,194,4,73,131,252,236,4,15,143,244,1,73,131,252,251,
  16,15,132,244,249,73,199,196,16,0,0,0,248,2,197,252,241,115,217,4,73,131,
  252,236,4,77,57,220,15,143,244,2,248,3,196,226,113,0,13,244,13,255,196,226,
  121,0,5,244,13,197,252,250,127,6,255,72,139,7,72,139,151,233,72,51,6,72,51,
  150,233,72,9,208,195,255,248,17,255,250,15,248,18,255,250,15,248,19,255,250,
  15,248,20,255,250,15,248,21,255,250,15,248,22,255,250,15,248,23,255,250,15,
  248,24,197,252,250,111,6,255,197,252,250,127,6,195,250,15,248,25,197,252,
  250,111,6,255,197,252,250,127,6,195,250,15,248,26,255,250,15,248,27,255,250,
  63,248,16,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,194,248,10,0,0,0,194,1,0,0,0,0,0,
  0,0,0,0,0,194,248,15,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,248,13,255,248,14,252,
  255,252,255,252,255,252,255,252,255,252,255,252,255,252,255,252,255,252,255,
  252,255,252,255,252,255,252,255,252,255,252,255,0,0,0,0,0,0,0,0,0,0,0,0,0,
  0,0,0,248,11,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,248,12,0,0,0,0,0,0,0,0,0,0,0,
  0,237,255
})

--|.globalnames globalnames
local globalnames = {
  [0] = "poly2",
  "one",
  "onef",
  "shuf_mask",
  "all_f",
  "two_one",
  "poly",
  "aesni_gcm_precomp_avx_gen4",
  "aes_keyexp_128_enc_avx",
  "aes_keyexp_256_enc_avx",
  "aesni_gcm_enc_128_avx_gen4",
  "aesni_gcm_enc_256_avx_gen4",
  "aesni_gcm_dec_128_avx_gen4",
  "aesni_gcm_dec_256_avx_gen4",
  "aesni_encrypt_128_single_block",
  "aesni_encrypt_256_single_block",
  "auth16_equal",
  "aad_prehash",
}

--|.define arg1, rdi
--|.define arg2, rsi
--|.define arg3, rdx
--|.define arg4, rcx
--|.define arg5, r8
--|.define arg6, r9
--|.define arg7, [r14 + 32 + 8*1]
--|.define arg8, [r14 + 32 + 8*2]
--|.define arg9, [r14 + 32 + 8*3]

local function ghash_tail(Dst, gh, t1, t2, t3)
  --| vmovdqa xmm(t3), [->poly2]
  --| vpclmulqdq xmm(t2), xmm(t3), xmm(gh), 0x01; vpslldq xmm(t2), xmm(t2), 8; vpxor xmm(gh), xmm(gh), xmm(t2)
  --| vpclmulqdq xmm(t2), xmm(t3), xmm(gh), 0x00; vpsrldq xmm(t2), xmm(t2), 4
  --| vpclmulqdq xmm(gh), xmm(t3), xmm(gh), 0x10; vpslldq xmm(gh), xmm(gh), 4; vpxor xmm(gh), xmm(gh), xmm(t2)
  dasm.put(Dst, 0, (t3), (t3), (t2), (gh), (t2), (t2), (gh), (gh), (t2), (t3), (t2), (gh), (t2), (t2), (t3), (gh), (gh))
  --| vpxor xmm(gh), xmm(gh), xmm(t1)
  dasm.put(Dst, 79, (gh), (gh), (gh), (gh), (t2), (gh), (gh), (t1))
end

local function ghash_mul(Dst, gh, hk, t1, t2, t3)
  --| vpclmulqdq xmm(t1), xmm(gh), xmm(hk), 0x11
  --| vpclmulqdq xmm(t2), xmm(gh), xmm(hk), 0x00
  --| vpclmulqdq xmm(t3), xmm(gh), xmm(hk), 0x01
  --| vpclmulqdq xmm(gh), xmm(gh), xmm(hk), 0x10
  --| vpxor xmm(gh), xmm(gh), xmm(t3)
  dasm.put(Dst, 115, (gh), (t1), (hk), (gh), (t2), (hk), (gh), (t3), (hk), (gh), (gh), (hk), (gh), (gh), (t3))

  --| vpsrldq xmm(t3), xmm(gh), 8
  --| vpslldq xmm(gh), xmm(gh), 8
  --| vpxor xmm(t1), xmm(t1), xmm(t3)
  --| vpxor xmm(gh), xmm(gh), xmm(t2)
  dasm.put(Dst, 176, (t3), (gh), (gh), (gh), (t1), (t1), (t3), (gh), (gh), (t2))
   ghash_tail(Dst, gh, t1, t2, t3)
end

local function almost_encrypt_8(Dst, initial, ctr, t_key, operation, loop_idx, nrounds, before_round)
  local prev = ctr
  for i = initial, 8 do
    if loop_idx == "in_order" then
      --| vpaddd xmm(i), xmm(prev), [->one]
      dasm.put(Dst, 222, (prev), (i))
    else
      --| vpaddd xmm(i), xmm(prev), [->onef]
      dasm.put(Dst, 235, (prev), (i))
    end
    prev = i
  end
  if prev ~= ctr then
    --| vmovdqa xmm(ctr), xmm(prev)
    dasm.put(Dst, 248, (ctr), (prev))
  end
  if loop_idx == "in_order" then
    for i = initial, 8 do
      --| vpshufb xmm(i), xmm(i), [->shuf_mask]
      dasm.put(Dst, 258, (i), (i))
    end
  end

  --| vmovdqa xmm(t_key), [arg1+16*0]
  dasm.put(Dst, 270, (t_key), 16*0)
  for i = initial, 8 do
    --| vpxor xmm(i), xmm(i), xmm(t_key)
    dasm.put(Dst, 102, (i), (i), (t_key))
  end
  for j = 1, nrounds do
    before_round(j)
    --| vmovdqa xmm(t_key), [arg1+16*j]
    dasm.put(Dst, 270, (t_key), 16*j)
    for i = initial, 8 do
      --| vaesenc xmm(i), xmm(i), xmm(t_key)
      dasm.put(Dst, 280, (i), (i), (t_key))
    end
  end
  before_round(nrounds+1)
end

local function encrypt_8(Dst, initial, t, ctr, t_key, operation, nrounds)
  almost_encrypt_8(Dst, initial, ctr, t_key, operation, "in_order", nrounds, function() end)

  --| vmovdqa xmm(t_key), [arg1+16*(nrounds+1)]
  dasm.put(Dst, 270, (t_key), 16*(nrounds+1))
  for i = initial, 8 do
    --| vaesenclast xmm(i), xmm(i), xmm(t_key)
    dasm.put(Dst, 292, (i), (i), (t_key))
  end

  for i = initial, 8 do
    --| vmovdqu xmm(t), [arg3 + r11 + 16*(i-initial)]
    --| vpxor xmm(i), xmm(i), xmm(t)
    --| vmovdqu [arg2 + r11 + 16*(i-initial)], xmm(i)
    dasm.put(Dst, 304, (t), 16*(i-initial), (i), (i), (t), (i), 16*(i-initial))
    if operation == "dec" then
      --| vmovdqa xmm(i), xmm(t)
      dasm.put(Dst, 248, (i), (t))
    end 
    --| vpshufb xmm(i), xmm(i), [->shuf_mask]
    dasm.put(Dst, 258, (i), (i))
  end
  --| add r11, (9-initial)*16
  dasm.put(Dst, 337, (9-initial)*16)
end

local function initial_blocks(Dst, num_initial_blocks, t, ctr, t_key, operation, nrounds)
  local i = 8 - num_initial_blocks
  --| vmovdqu xmm(i), [arg6]
  --| vpshufb xmm(i), xmm(i), [->shuf_mask]
  dasm.put(Dst, 342, (i), (i), (i))

  --| xor r11, r11
  --| mov rax, arg5
  --| vmovdqu xmm(ctr), [rax]
  --| vpshufb xmm(ctr), xmm(ctr), [->shuf_mask]
  dasm.put(Dst, 361, (ctr), (ctr), (ctr))
   encrypt_8(Dst, 9-num_initial_blocks, t[1], ctr, t_key, operation, nrounds)

  local prev
  --| vmovdqu xmm(t[2]), [arg1 + 16*15]
  dasm.put(Dst, 386, (t[2]), 16*15)
  for j = 8-num_initial_blocks, 8 do
    if prev then
      --| vpxor xmm(j), xmm(j), xmm(prev)
      dasm.put(Dst, 102, (j), (j), (prev))
    end
    ghash_mul(Dst, j, t[2], t[1], t[3], t[4])
    prev = j
  end

  --| vmovdqa [rsp], xmm8
  --| vmovdqa xmm(t[3]), xmm8
  --| cmp r13, 128
  --| jl >9
  dasm.put(Dst, 396, (t[3]))
   encrypt_8(Dst, 1, t[1], ctr, t_key, operation, nrounds)
  --| vpxor xmm1, xmm1, [rsp]
  --|9:
  dasm.put(Dst, 421)
end

local function mulqdqxor(Dst, out, qdq1, qdq2, qdqI, xor)
  --| vpclmulqdq xmm(xor or out), xmm(qdq1), xmm(qdq2), qdqI
  dasm.put(Dst, 431, (qdq1), (xor or out), (qdq2), qdqI)
  if xor then
    --| vpxor xmm(out), xmm(out), xmm(xor)
    dasm.put(Dst, 102, (out), (out), (xor))
  end
end

local function ghash_8_encrypt_8_parallel(Dst, t, ctr, loop_idx, operation, nrounds)
  --| add r15b, 8
  --| vmovdqa xmm(t[2]), xmm1
  dasm.put(Dst, 444, (t[2]))
  for i = 2, 8 do
    --| vmovdqa [rsp + 16*(i-1)], xmm(i)
    dasm.put(Dst, 456, (i), 16*(i-1))
  end

  almost_encrypt_8(Dst, 1, ctr, t[1], operation, loop_idx, nrounds, function(round)
    -- This has nothing to do with the round per se, its just splicing in
    -- parallel ghash work (eight iterations, operating on shifted_key_8-1).
    local start = nrounds - 6
    local i = round - start -- 0-7
    if round >= start then
      --| vmovdqa xmm(t[5]), [arg1 + 16*(22-i)]
      dasm.put(Dst, 270, (t[5]), 16*(22-i))
      local xor
      if round > (nrounds - 6) then
        --| vmovdqa xmm(t[2]), [rsp + 16*(i)]
        dasm.put(Dst, 467, (t[2]), 16*(i))
        xor = t[3]
      end
      mulqdqxor(Dst, t[4], t[2], t[5], 0x11, xor)
      mulqdqxor(Dst, t[7], t[2], t[5], 0x00, xor)
      mulqdqxor(Dst, t[6], t[2], t[5], 0x01, xor)
      mulqdqxor(Dst, t[6], t[2], t[5], 0x10, t[3])
    end
  end)

  --| vmovdqa xmm(t[5]), [arg1+16*(nrounds+1)]
  dasm.put(Dst, 270, (t[5]), 16*(nrounds+1))
  for j = 1, 8 do
    local i = j - 1
    --| vpxor xmm(t[2]), xmm(t[5]), [arg3 + r11 + 16*i]
    dasm.put(Dst, 478, (t[5]), (t[2]), 16*i)
    if operation == "enc" then
      --| vaesenclast xmm(j), xmm(j), xmm(t[2])
      --| vmovdqu [arg2 + r11 + 16*i], xmm(j)
      dasm.put(Dst, 492, (j), (j), (t[2]), (j), 16*i)
    else
      --| vaesenclast xmm(t[3]), xmm(j), xmm(t[2])
      --| vmovdqu xmm(j), [arg3 + r11 + 16*i]
      --| vmovdqu [arg2 + r11 + 16*i], xmm(t[3])
      dasm.put(Dst, 514, (j), (t[3]), (t[2]), (j), 16*i, (t[3]), 16*i)
    end
    --| vpshufb xmm(j), xmm(j), [->shuf_mask]
    dasm.put(Dst, 258, (j), (j))
  end

  --| vpslldq xmm(t[3]), xmm(t[6]), 8
  --| vpsrldq xmm(t[6]), xmm(t[6]), 8
  --| vpxor xmm(t[7]), xmm(t[7]), xmm(t[3])
  --| vpxor xmm(t[1]), xmm(t[4]), xmm(t[6])
  dasm.put(Dst, 546, (t[3]), (t[6]), (t[6]), (t[6]), (t[7]), (t[7]), (t[3]), (t[4]), (t[1]), (t[6]))
   ghash_tail(Dst, t[7], t[1], t[2], t[3])
  --| vpxor xmm1, xmm1, xmm(t[7])
  --| add r11, 128
  --| sub r13, 128
  dasm.put(Dst, 592, (t[7]))
end

local function ghash_last_8(Dst, t)
  for i = 1, 8 do
    --| vmovdqa xmm(t[5]), [arg1 + 16*(23-i)]
    --| vpshufd xmm(t[2]), xmm(i), 0x4e
    --| vpshufd xmm(t[3]), xmm(t[5]), 0x4e
    --| vpxor xmm(t[2]), xmm(t[2]), xmm(i)
    --| vpxor xmm(t[3]), xmm(t[3]), xmm(t[5])
    dasm.put(Dst, 616, (t[5]), 16*(23-i), (t[2]), (i), (t[3]), (t[5]), (t[2]), (t[2]), (i), (t[3]), (t[3]), (t[5]))
    mulqdqxor(Dst, t[6], i, t[5], 0x11, i ~= 1 and t[4])
    mulqdqxor(Dst, t[7], i, t[5], 0x00, i ~= 1 and t[4])
    mulqdqxor(Dst, 1, t[2], t[3], 0x00, i ~= 1 and t[4])
  end
  --| vpxor xmm1, xmm1, xmm(t[6])
  --| vpxor xmm(t[2]), xmm1, xmm(t[7])
  dasm.put(Dst, 670, (t[6]), (t[2]), (t[7]))

  --| vpslldq xmm(t[4]), xmm(t[2]), 8
  --| vpsrldq xmm(t[2]), xmm(t[2]), 8
  --| vpxor xmm(t[7]), xmm(t[7]), xmm(t[4])
  --| vpxor xmm(t[6]), xmm(t[6]), xmm(t[2])
  dasm.put(Dst, 546, (t[4]), (t[2]), (t[2]), (t[2]), (t[7]), (t[7]), (t[4]), (t[6]), (t[6]), (t[2]))
   ghash_tail(Dst, t[7], t[6], t[2], t[3])
  --| vmovdqa xmm14, xmm15
  dasm.put(Dst, 689)
end

local function encrypt_single_block(Dst, x, nrounds)
  --| vpxor xmm(x), xmm(x), [arg1+16*0]
  dasm.put(Dst, 696, (x), (x), 16*0)
  for i = 1, nrounds do
    --| vaesenc xmm(x), xmm(x), [arg1+16*i]
    dasm.put(Dst, 709, (x), (x), 16*i)
  end
  --| vaesenclast xmm(x), xmm(x), [arg1+16*(nrounds+1)]
  dasm.put(Dst, 721, (x), (x), 16*(nrounds+1))
end

local function prologue(Dst)
  for i = 12, 15 do
    --| push Rq(i)
    dasm.put(Dst, 733, (i))
  end
  --| mov r14, rsp
  --| sub rsp, 16*8
  --| and rsp, -64
  dasm.put(Dst, 738, 16*8)
end

local function epilogue(Dst)
  --| mov rsp, r14
  dasm.put(Dst, 751)
  for i = 15, 12, -1 do
    --| pop Rq(i)
    dasm.put(Dst, 756, (i))
  end
  --| ret
  dasm.put(Dst, 761)
end

local function gcm_enc_dec(Dst, operation, pc, nrounds)
  prologue(Dst)

  --| mov r13, arg4
  --| and r13, -16
  --| mov r12, r13
  --| shr r12, 4
  --| and r12, 7
  --| jz =>pc+0
  dasm.put(Dst, 763, pc+0)
  for i = 7, 2, -1 do
    --| cmp r12, i
    --| je =>pc+i
    dasm.put(Dst, 788, i, pc+i)
  end
  --| jmp =>pc+1
  dasm.put(Dst, 797, pc+1)
  for i = 7, 0, -1 do
    --|=>pc+i:
    dasm.put(Dst, 801, pc+i)
     initial_blocks(Dst, i, {12, 13, 14, 15}, 9, 0, operation, nrounds)
    if i ~= 0 then
      --| sub r13, 16*i
      --| jmp >8
      dasm.put(Dst, 803, 16*i)
    end
  end

  --|8:
  --| cmp r13, 0
  --| je >1
  --| sub r13, 128
  --| je >2
  --| vmovd r15d, xmm9
  --| and r15d, 255
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  --|3:
  --| cmp r15b, 255-8
  --| jg >4
  dasm.put(Dst, 813, 255-8)
   ghash_8_encrypt_8_parallel(Dst, {0, 10, 11, 12, 13, 14, 15}, 9, "out_order", operation, nrounds)
  --| jne <3
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  --| jmp >2
  --|4:
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  dasm.put(Dst, 868)
   ghash_8_encrypt_8_parallel(Dst, {0, 10, 11, 12, 13, 14, 15}, 9, "in_order", operation, nrounds)
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  --| jne <3
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  --|2:
  dasm.put(Dst, 893)
   ghash_last_8(Dst, {0, 10, 11, 12, 13, 14, 15})
  --|1:
  dasm.put(Dst, 914)

  --| mov r13, arg4
  --| and r13, 15
  --| je >1
  dasm.put(Dst, 917)

  --| vpaddd xmm9, xmm9, [->one]
  --| vpshufb xmm9, xmm9, [->shuf_mask]
  dasm.put(Dst, 929)
   encrypt_single_block(Dst, 9, nrounds)

  --| sub r11, 16
  --| add r11, r13
  --| vmovdqu xmm1, [arg3 + r11]
  --| lea r12, [->all_f]
  --| sub r12, r13
  --| vmovdqu xmm2, [r12]
  --| vpshufb xmm1, xmm1, xmm2
  dasm.put(Dst, 944)

  if operation == "dec" then
    --| vmovdqa xmm2, xmm1
    dasm.put(Dst, 980)
  end
  --| vpxor xmm9, xmm9, xmm1
  --| vmovdqu xmm1, [r12 + 16]
  --| vpand xmm9, xmm9, xmm1
  dasm.put(Dst, 986, 16)
  if operation == "dec" then
    --| vpand xmm2, xmm2, xmm1
    dasm.put(Dst, 1004)
  else
    --| vmovdqa xmm2, xmm9
    dasm.put(Dst, 1010)
  end
  --| vpshufb xmm2, xmm2, [->shuf_mask]
  --| vpxor xmm14, xmm14, xmm2
  dasm.put(Dst, 1016)
   ghash_mul(Dst, 14, 13, 0, 10, 11)
  --| sub r11, r13
  --| add r11, 16
  dasm.put(Dst, 1030)

  --| vmovd rax, xmm9
  --| cmp r13, 8
  --| jle >2
  --| mov [arg2 + r11], rax
  --| add r11, 8
  --| vpsrldq xmm9, xmm9, 8
  --| vmovd rax, xmm9
  --| sub r13, 8
  --|2:
  --| mov byte [arg2 + r11], al
  --| add r11, 1
  --| shr rax, 8
  --| sub r13, 1
  --| jne <2
  dasm.put(Dst, 1039)

  --|1:
  --| mov r12, arg7
  --| shl r12, 3
  --| vmovd xmm15, r12d
  dasm.put(Dst, 1103, 32 + 8*1)

  --| shl arg4, 3
  --| vmovd xmm1, arg4
  --| vpslldq xmm15, xmm15, 8
  --| vpxor xmm15, xmm15, xmm1
  dasm.put(Dst, 1120)
        
  --| vpxor xmm14, xmm14, xmm15
  dasm.put(Dst, 1144)
   ghash_mul(Dst, 14, 13, 0, 10, 11)
  --| vpshufb xmm14, xmm14, [->shuf_mask]
  --| mov rax, arg5
  --| vmovdqu xmm9, [rax]
  dasm.put(Dst, 1152)
   encrypt_single_block(Dst, 9, nrounds)
  --| vpxor xmm9, xmm9, xmm14
  dasm.put(Dst, 1167)

  --| mov r10, arg8
  --| mov r11, arg9
  --| cmp r11, 16
  --| je >3
  --| cmp r11, 12
  --| je >2
  --| vmovd rax, xmm9
  --| mov [r10], rax
  --| jmp >4
  --|2:
  --| vmovd rax, xmm9
  --| mov [r10], rax
  --| vpsrldq xmm9, xmm9, 8
  --| vmovd eax, xmm9
  --| mov [r10 + 8], eax
  --| jmp >4       
  --|3:
  --| vmovdqu [r10], xmm9
  --|4:
  dasm.put(Dst, 1174, 32 + 8*2, 32 + 8*3, 8)

  epilogue(Dst)
end

local function precompute(Dst)
  prologue(Dst)

  --| vmovdqu xmm6, [arg2]
  --| vpshufb xmm6, xmm6, [->shuf_mask]
  --| vmovdqa xmm2, xmm6
  --| vpsllq xmm6, xmm6, 1
  --| vpsrlq xmm2, xmm2, 63
  --| vmovdqa xmm1, xmm2
  --| vpslldq xmm2, xmm2, 8 
  --| vpsrldq xmm1, xmm1, 8
  --| vpor xmm6, xmm6, xmm2     
  --| vpshufd xmm2, xmm1, 0x24
  --| vpcmpeqd xmm2, xmm2, [->two_one]
  --| vpand xmm2, xmm2, [->poly]
  --| vpxor xmm6, xmm6, xmm2
  --| vmovdqa [arg1 + 16*15], xmm6
  dasm.put(Dst, 1252, 16*15)

  --| vmovdqa xmm4, xmm6
  dasm.put(Dst, 1338)
  for i = 2, 8 do
     ghash_mul(Dst, 4, 6, 0, 1, 2)
    --| vmovdqa [arg1 + 16*(14+i)], xmm4
    dasm.put(Dst, 1344, 16*(14+i))
  end

  epilogue(Dst)
end

local function keyexp_round (Dst, round)
  --| vpshufd xmm2, xmm2, 0xff
  --| vshufps xmm3, xmm3, xmm1, 0x10
  --| vpxor xmm1, xmm1, xmm3
  --| vshufps xmm3, xmm3, xmm1, 0x8c
  --| vpxor xmm1, xmm1, xmm3
  --| vpxor xmm1, xmm1, xmm2
  dasm.put(Dst, 1351)
end

local function keyexp2_round (Dst, round)
  --| vpshufd xmm2, xmm2, 0xaa
  --| vshufps xmm3, xmm3, xmm4, 0x10
  --| vpxor xmm4, xmm4, xmm3
  --| vshufps xmm3, xmm3, xmm4, 0x8c
  --| vpxor xmm4, xmm4, xmm3
  --| vpxor xmm4, xmm4, xmm2
  dasm.put(Dst, 1387)
end

local function keyexp128(Dst)
  --| vmovdqu xmm1, [arg1]
  --| vmovdqa [arg2], xmm1
  --| vpxor xmm3, xmm3, xmm3
  dasm.put(Dst, 1419)
  for i = 1, 10 do
    --| vaeskeygenassist xmm2, xmm1, i < 9 and 2^(i-1) or 27*(i-8)
    dasm.put(Dst, 1435, i < 9 and 2^(i-1) or 27*(i-8))
     keyexp_round(Dst)
    --| vmovdqa [arg2 + 16*i], xmm1
    dasm.put(Dst, 1442, 16*i)
  end
  --| ret
  dasm.put(Dst, 761)
end

local function keyexp256(Dst)
  --| vmovdqu xmm1, [arg1]
  --| vmovdqa [arg2], xmm1
  --| vmovdqu xmm4, [arg1+16]
  --| vmovdqa [arg2+16], xmm4
  --| vpxor xmm3, xmm3, xmm3
  dasm.put(Dst, 1449)
  for i = 1, 6 do
    --| vaeskeygenassist xmm2, xmm4, 2^(i-1)
    dasm.put(Dst, 1477, 2^(i-1))
     keyexp_round(Dst)
    --| vmovdqa [arg2 + 16*(i*2)], xmm1
    --| vaeskeygenassist xmm2, xmm1, 2^(i-1)
    dasm.put(Dst, 1484, 16*(i*2), 2^(i-1))
     keyexp2_round(Dst)
    --| vmovdqa [arg2 + 16*(i*2+1)], xmm4
    dasm.put(Dst, 1497, 16*(i*2+1))
  end
  --| vaeskeygenassist xmm2, xmm4, 0x40 -- 2^(7-1)
  dasm.put(Dst, 1504)
   keyexp_round(Dst)
  --| vmovdqa [arg2 + 16*14], xmm1
  --| ret
  dasm.put(Dst, 1511, 16*14)
end

local function aad_prehash(Dst)
   prologue(Dst)
   --| vmovdqu xmm2, [arg1 + 16*15]
   --| mov r10, arg3
   --| mov r12, arg4
   dasm.put(Dst, 1519, 16*15)

   --| vpxor xmm0, xmm0, xmm0
   dasm.put(Dst, 1532)

   --|1:
   --| cmp r12, 16
   --| jng >2
   --| vmovdqu xmm1, [r10]
   --| vpshufb xmm1, xmm1, [->shuf_mask]
   --| vpxor xmm0, xmm0, xmm1
   dasm.put(Dst, 1539)
   ghash_mul(Dst, 0, 2, 13, 14, 15)
   --| add r10, 16
   --| sub r12, 16
   --| jmp <1
   dasm.put(Dst, 1569)

   --|2:
   --| mov r11, r12
   --| vpxor xmm1, xmm1, xmm1
   --|1:
   --| vmovd xmm3, dword [r10]
   --| vpslldq xmm3, xmm3, 12
   --| vpsrldq xmm1, xmm1, 4
   --| vpxor xmm1, xmm1, xmm3
   --| add r10, 4
   --| sub r12, 4
   --| jg <1
   --| cmp r11, 16
   --| je >3
   --| mov r12, 16
   --|2:
   --| vpsrldq xmm1, xmm1, 4
   --| sub r12, 4
   --| cmp r12, r11
   --| jg <2
   --|3:
   --| vpshufb xmm1, xmm1, [->shuf_mask]
   dasm.put(Dst, 1583)

   --| vpxor xmm0, xmm0, xmm1
   dasm.put(Dst, 1562)

   --| vpshufb xmm0, xmm0, [->shuf_mask] -- undone by initial_blocks
   --| vmovdqu [arg2], xmm0
   dasm.put(Dst, 1678)
   epilogue(Dst)
end

local function auth16_equal(Dst)
   --| mov rax, [arg1]
   --| mov rdx, [arg1 + 8]
   --| xor rax, [arg2]
   --| xor rdx, [arg2 + 8]
   --| or rax, rdx
   --| ret
   dasm.put(Dst, 1691, 8, 8)
end

local function generator(Dst)
  Dst:growpc(32)

  -- Functions
  --|->aesni_gcm_precomp_avx_gen4:
  dasm.put(Dst, 1710)
   precompute(Dst)
  --|.align 16
  --|->aes_keyexp_128_enc_avx:
  dasm.put(Dst, 1713)
   keyexp128(Dst)
  --|.align 16
  --|->aes_keyexp_256_enc_avx:
  dasm.put(Dst, 1718)
   keyexp256(Dst)
  --|.align 16
  --|->aesni_gcm_enc_128_avx_gen4:
  dasm.put(Dst, 1723)
   gcm_enc_dec(Dst, "enc", 0, 9)
  --|.align 16
  --|->aesni_gcm_enc_256_avx_gen4:
  dasm.put(Dst, 1728)
   gcm_enc_dec(Dst, "enc", 8, 13)
  --|.align 16
  --|->aesni_gcm_dec_128_avx_gen4:
  dasm.put(Dst, 1733)
   gcm_enc_dec(Dst, "dec", 16, 9)
  --|.align 16
  --|->aesni_gcm_dec_256_avx_gen4:
  dasm.put(Dst, 1738)
   gcm_enc_dec(Dst, "dec", 24, 13)
  --|.align 16
  --|->aesni_encrypt_128_single_block:
  --| vmovdqu xmm0, [arg2]
  dasm.put(Dst, 1743)
   encrypt_single_block(Dst, 0, 9)
  --| vmovdqu [arg2], xmm0
  --| ret
  --|.align 16
  --|->aesni_encrypt_256_single_block:
  --| vmovdqu xmm0, [arg2]
  dasm.put(Dst, 1753)
   encrypt_single_block(Dst, 0, 13)
  --| vmovdqu [arg2], xmm0
  --| ret
  --|.align 16
  --|->auth16_equal:
  dasm.put(Dst, 1769)
   auth16_equal(Dst)
  --|.align 16
  --|->aad_prehash:
  dasm.put(Dst, 1780)
   aad_prehash(Dst)

  -- Data
  --|.align 64
  --|->poly:;    .dword          1, 0, 0, 0xC2000000
  --|->poly2:;   .dword 0xC2000000, 1, 0, 0xC2000000
  --|->two_one:; .dword          1, 0, 0,          1
  --|->shuf_mask:
  dasm.put(Dst, 1785)
  for i = 15, 0, -1 do
    --|.byte i
    dasm.put(Dst, 442, i)
  end
  for i = 0, 15 do
    --|.byte i
    dasm.put(Dst, 442, i)
  end
  --|->all_f:; .dword -1, -1, -1,   -1
  --|          .dword  0,  0,  0,    0
  --|->one:;   .dword  1,  0,  0,    0
  --|->onef:;  .dword  0,  0,  0, 2^24
  dasm.put(Dst, 1844, 2^24)
end

-- USAGE
--
-- First allocate and initialize a gcm_data state using the encryption key and
-- a derived hash subkey. This is done i.e. like so:
--
--   gcm_data = ffi.new("gcm_data __attribute__((aligned(16)))")
--   aes_keyexp_128_enc_avx(key, gcm_data)
--   (or aes_keyexp_256_enc_avx)
--   hash_subkey = ffi.new("uint8_t[16]")
--   aesni_encrypt_128_single_block(gcm_data, hash_subkey)
--   (or aesni_encrypt_256_single_block)
--   aesni_gcm_precomp_avx_gen4(gcm_data, hash_subkey)
-- 
-- Then your gcm_data state is ready for use with aesni_gcm_enc_*_avx_gen4,
-- aesni_gcm_dec_*_avx_gen4, and aad_prehash.
--
-- If your AAD is <= 16 bytes you can pad it to 16 bytes with zero bits and
-- pass it to aesni_gcm_enc_*_avx_gen4 and aesni_gcm_dec_*_avx_gen4 as is.
-- However, if your AAD exceeds 16 bytes you need to pre-hash it using
-- aad_prehash and use the resulting 16 byte output. You still need to pass the
-- actual length of the AAD (i.e., as supplied to aad_prehash) to
-- aesni_gcm_enc_*_avx_gen4 and aesni_gcm_dec_*_avx_gen4!

-- You can also use aad_prehash on <= 16 byte AAD, but that amounts to a NOOP.
-- Either way, the AAD’s length MUST be a multiple of four (i.e., not zero).
--
-- Make sure to check the resulting authentication tags, i.e. using
-- auth16_equal!
--
-- Arguments to aes_keyexp_128_enc:
--  1) uint8_t  key[16]
--  2) gcm_data *state       (aligned to 16 bytes)
--
-- Arguments to aes_keyexp_256_enc:
--  1) uint8_t  key[32]
--  2) gcm_data *state       (aligned to 16 bytes)
--
-- Arguments to aesni_encrypt_128_single_block, aesni_encrypt_256_single_block:
--  1) gcm_data *state       (aligned to 16 bytes)
--  2) uint8_t block[16]
--
-- Arguments to aesni_gcm_precomp_avx_gen4:
--  1) gcm_data *state       (aligned to 16 bytes)
--  2) uint8_t  hash_subkey[16]
--
-- Arguments to aad_prehash:
--  1) gcm_data *state      (aligned to 16 bytes)
--  2) uint8_t  out[16]
--  3) uint8_t  aad[aadlen] (AAD material)
--  4) uint64_t aadlen      (a multiple of 4)
--
-- Arguments to aesni_gcm_enc_128_avx_gen4, aesni_gcm_dec_128_avx_gen4,
-- aesni_gcm_enc_256_avx_gen4, aesni_gcm_dec_256_avx_gen4:
--  1) gcm_data *state      (aligned to 16 bytes)
--  2) uint8_t  out[len]
--  3) uint8_t  in[len]     (can be the same as out, i.e. "in-place")
--  4) uint64_t len
--  5) uint8_t  iv[16]      (96 bit IV, padded with bytes 0001, aligned to 16 bytes)
--  6) uint8_t  aad[16]     (16 bytes, output of aad_prehash if aadlen > 16)
--  7) uint64_t aadlen      (a multiple of 4, the actual aadlen as passed to aad_prehash)
--  8) uint8_t  tag[taglen]
--  9) uint64_t taglen      (should be 16 for all intents and purposes)
--
-- Arguments to auth16_equal:
--  1) uint8_t x[16]
--  2) uint8_t y[16]
-- Returns:
--  zero only if x and y are equal.

local Dst, globals = dasm.new(actions, nil, nil, 1 + #globalnames)
generator(Dst)
local mcode, size = Dst:build()
local entry = dasm.globals(globals, globalnames)
local fn_t = ffi.typeof("void(*)(gcm_data*, uint8_t*, const uint8_t*, uint64_t, const uint8_t*, const uint8_t*, uint64_t, uint8_t*, uint64_t)")
return setmetatable({
  aes_keyexp_128_enc_avx = ffi.cast("void(*)(const uint8_t*, gcm_data*)", entry.aes_keyexp_128_enc_avx),
  aes_keyexp_256_enc_avx = ffi.cast("void(*)(const uint8_t*, gcm_data*)", entry.aes_keyexp_256_enc_avx),
  aesni_gcm_precomp_avx_gen4 = ffi.cast("void(*)(gcm_data*, const uint8_t*)", entry.aesni_gcm_precomp_avx_gen4),
  aad_prehash = ffi.cast("void(*)(gcm_data*, uint8_t[16], const uint8_t*, uint64_t)", entry.aad_prehash),
  aesni_gcm_enc_128_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_enc_128_avx_gen4),
  aesni_gcm_enc_256_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_enc_256_avx_gen4),
  aesni_gcm_dec_128_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_dec_128_avx_gen4),
  aesni_gcm_dec_256_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_dec_256_avx_gen4),
  aesni_encrypt_128_single_block = ffi.cast("void(*)(gcm_data*, uint8_t*)", entry.aesni_encrypt_128_single_block),
  aesni_encrypt_256_single_block = ffi.cast("void(*)(gcm_data*, uint8_t*)", entry.aesni_encrypt_256_single_block),
  auth16_equal = ffi.cast("uint64_t(*)(uint8_t[16], uint8_t[16])", entry.auth16_equal)
}, {_anchor = mcode})
