-- Use of this source code is governed by the Apache 2.0 license; see COPYING.

-- Selected AES GCM routines, based heavily on the Intel IPsec code from:
-- https://github.com/lukego/intel-ipsec/blob/master/code/avx2/gcm_avx_gen4.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/gcm_defines.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/aes_keyexp_128.asm
-- https://github.com/lukego/intel-ipsec/blob/master/code/aes_keyexp_256.asm

local dasm = require("dasm")
local ffi = require("ffi")

ffi.cdef[[
typedef struct gcm_data
{
  uint8_t expanded_keys[16*15];
  uint8_t shifted_hkey_1[16];
  uint8_t shifted_hkey_2[16];
  uint8_t shifted_hkey_3[16];
  uint8_t shifted_hkey_4[16];
  uint8_t shifted_hkey_5[16];
  uint8_t shifted_hkey_6[16];
  uint8_t shifted_hkey_7[16];
  uint8_t shifted_hkey_8[16];
  uint8_t shifted_hkey_1_k[16];
  uint8_t shifted_hkey_2_k[16];
  uint8_t shifted_hkey_3_k[16];
  uint8_t shifted_hkey_4_k[16];
  uint8_t shifted_hkey_5_k[16];
  uint8_t shifted_hkey_6_k[16];
  uint8_t shifted_hkey_7_k[16];
  uint8_t shifted_hkey_8_k[16];
} gcm_data;
]]

|.arch x64
|.actionlist actions
|.globalnames globalnames

|.define arg1, rdi
|.define arg2, rsi
|.define arg3, rdx
|.define arg4, rcx
|.define arg5, r8
|.define arg6, r9
|.define arg7, [r14 + 32 + 8*1]
|.define arg8, [r14 + 32 + 8*2]
|.define arg9, [r14 + 32 + 8*3]

local function ghash_tail(Dst, gh, t1, t2, t3)
  | vmovdqa xmm(t3), [->poly2]
  | vpclmulqdq xmm(t2), xmm(t3), xmm(gh), 0x01; vpslldq xmm(t2), xmm(t2), 8; vpxor xmm(gh), xmm(gh), xmm(t2)
  | vpclmulqdq xmm(t2), xmm(t3), xmm(gh), 0x00; vpsrldq xmm(t2), xmm(t2), 4
  | vpclmulqdq xmm(gh), xmm(t3), xmm(gh), 0x10; vpslldq xmm(gh), xmm(gh), 4; vpxor xmm(gh), xmm(gh), xmm(t2)
  | vpxor xmm(gh), xmm(gh), xmm(t1)
end

local function ghash_mul(Dst, gh, hk, t1, t2, t3)
  | vpclmulqdq xmm(t1), xmm(gh), xmm(hk), 0x11
  | vpclmulqdq xmm(t2), xmm(gh), xmm(hk), 0x00
  | vpclmulqdq xmm(t3), xmm(gh), xmm(hk), 0x01
  | vpclmulqdq xmm(gh), xmm(gh), xmm(hk), 0x10
  | vpxor xmm(gh), xmm(gh), xmm(t3)

  | vpsrldq xmm(t3), xmm(gh), 8
  | vpslldq xmm(gh), xmm(gh), 8
  | vpxor xmm(t1), xmm(t1), xmm(t3)
  | vpxor xmm(gh), xmm(gh), xmm(t2)
  || ghash_tail(Dst, gh, t1, t2, t3)
end

local function almost_encrypt_8(Dst, initial, ctr, t_key, operation, loop_idx, nrounds, before_round)
  local prev = ctr
  for i = initial, 8 do
    if loop_idx == "in_order" then
      | vpaddd xmm(i), xmm(prev), [->one]
    else
      | vpaddd xmm(i), xmm(prev), [->onef]
    end
    prev = i
  end
  if prev ~= ctr then
    | vmovdqa xmm(ctr), xmm(prev)
  end
  if loop_idx == "in_order" then
    for i = initial, 8 do
      | vpshufb xmm(i), xmm(i), [->shuf_mask]
    end
  end

  | vmovdqa xmm(t_key), [arg1+16*0]
  for i = initial, 8 do
    | vpxor xmm(i), xmm(i), xmm(t_key)
  end
  for j = 1, nrounds do
    before_round(j)
    | vmovdqa xmm(t_key), [arg1+16*j]
    for i = initial, 8 do
      | vaesenc xmm(i), xmm(i), xmm(t_key)
    end
  end
  before_round(nrounds+1)
end

local function encrypt_8(Dst, initial, t, ctr, t_key, operation, nrounds)
  almost_encrypt_8(Dst, initial, ctr, t_key, operation, "in_order", nrounds, function() end)

  | vmovdqa xmm(t_key), [arg1+16*(nrounds+1)]
  for i = initial, 8 do
    | vaesenclast xmm(i), xmm(i), xmm(t_key)
  end

  for i = initial, 8 do
    | vmovdqu xmm(t), [arg3 + r11 + 16*(i-initial)]
    | vpxor xmm(i), xmm(i), xmm(t)
    | vmovdqu [arg2 + r11 + 16*(i-initial)], xmm(i)
    if operation == "dec" then
      | vmovdqa xmm(i), xmm(t)
    end 
    | vpshufb xmm(i), xmm(i), [->shuf_mask]
  end
  | add r11, (9-initial)*16
end

local function initial_blocks(Dst, num_initial_blocks, t, ctr, t_key, operation, nrounds)
  local i = 8 - num_initial_blocks
  | vmovdqu xmm(i), [arg6]
  | vpshufb xmm(i), xmm(i), [->shuf_mask]

  | xor r11, r11
  | mov rax, arg5
  | vmovdqu xmm(ctr), [rax]
  | vpshufb xmm(ctr), xmm(ctr), [->shuf_mask]
  || encrypt_8(Dst, 9-num_initial_blocks, t[1], ctr, t_key, operation, nrounds)

  local prev
  | vmovdqu xmm(t[2]), [arg1 + 16*15]
  for j = 8-num_initial_blocks, 8 do
    if prev then
      | vpxor xmm(j), xmm(j), xmm(prev)
    end
    ghash_mul(Dst, j, t[2], t[1], t[3], t[4])
    prev = j
  end

  | vmovdqa [rsp], xmm8
  | vmovdqa xmm(t[3]), xmm8
  | cmp r13, 128
  | jl >9
  || encrypt_8(Dst, 1, t[1], ctr, t_key, operation, nrounds)
  | vpxor xmm1, xmm1, [rsp]
  |9:
end

local function mulqdqxor(Dst, out, qdq1, qdq2, qdqI, xor)
  | vpclmulqdq xmm(xor or out), xmm(qdq1), xmm(qdq2), qdqI
  if xor then
    | vpxor xmm(out), xmm(out), xmm(xor)
  end
end

local function ghash_8_encrypt_8_parallel(Dst, t, ctr, loop_idx, operation, nrounds)
  | add r15b, 8
  | vmovdqa xmm(t[2]), xmm1
  for i = 2, 8 do
    | vmovdqa [rsp + 16*(i-1)], xmm(i)
  end

  almost_encrypt_8(Dst, 1, ctr, t[1], operation, loop_idx, nrounds, function(round)
    -- This has nothing to do with the round per se, its just splicing in
    -- parallel ghash work (eight iterations, operating on shifted_key_8-1).
    local start = nrounds - 6
    local i = round - start -- 0-7
    if round >= start then
      | vmovdqa xmm(t[5]), [arg1 + 16*(22-i)]
      local xor
      if round > (nrounds - 6) then
        | vmovdqa xmm(t[2]), [rsp + 16*(i)]
        xor = t[3]
      end
      mulqdqxor(Dst, t[4], t[2], t[5], 0x11, xor)
      mulqdqxor(Dst, t[7], t[2], t[5], 0x00, xor)
      mulqdqxor(Dst, t[6], t[2], t[5], 0x01, xor)
      mulqdqxor(Dst, t[6], t[2], t[5], 0x10, t[3])
    end
  end)

  | vmovdqa xmm(t[5]), [arg1+16*(nrounds+1)]
  for j = 1, 8 do
    local i = j - 1
    | vpxor xmm(t[2]), xmm(t[5]), [arg3 + r11 + 16*i]
    if operation == "enc" then
      | vaesenclast xmm(j), xmm(j), xmm(t[2])
      | vmovdqu [arg2 + r11 + 16*i], xmm(j)
    else
      | vaesenclast xmm(t[3]), xmm(j), xmm(t[2])
      | vmovdqu xmm(j), [arg3 + r11 + 16*i]
      | vmovdqu [arg2 + r11 + 16*i], xmm(t[3])
    end
    | vpshufb xmm(j), xmm(j), [->shuf_mask]
  end

  | vpslldq xmm(t[3]), xmm(t[6]), 8
  | vpsrldq xmm(t[6]), xmm(t[6]), 8
  | vpxor xmm(t[7]), xmm(t[7]), xmm(t[3])
  | vpxor xmm(t[1]), xmm(t[4]), xmm(t[6])
  || ghash_tail(Dst, t[7], t[1], t[2], t[3])
  | vpxor xmm1, xmm1, xmm(t[7])
  | add r11, 128
  | sub r13, 128
end

local function ghash_last_8(Dst, t)
  for i = 1, 8 do
    | vmovdqa xmm(t[5]), [arg1 + 16*(23-i)]
    | vpshufd xmm(t[2]), xmm(i), 0x4e
    | vpshufd xmm(t[3]), xmm(t[5]), 0x4e
    | vpxor xmm(t[2]), xmm(t[2]), xmm(i)
    | vpxor xmm(t[3]), xmm(t[3]), xmm(t[5])
    mulqdqxor(Dst, t[6], i, t[5], 0x11, i ~= 1 and t[4])
    mulqdqxor(Dst, t[7], i, t[5], 0x00, i ~= 1 and t[4])
    mulqdqxor(Dst, 1, t[2], t[3], 0x00, i ~= 1 and t[4])
  end
  | vpxor xmm1, xmm1, xmm(t[6])
  | vpxor xmm(t[2]), xmm1, xmm(t[7])

  | vpslldq xmm(t[4]), xmm(t[2]), 8
  | vpsrldq xmm(t[2]), xmm(t[2]), 8
  | vpxor xmm(t[7]), xmm(t[7]), xmm(t[4])
  | vpxor xmm(t[6]), xmm(t[6]), xmm(t[2])
  || ghash_tail(Dst, t[7], t[6], t[2], t[3])
  | vmovdqa xmm14, xmm15
end

local function encrypt_single_block(Dst, x, nrounds)
  | vpxor xmm(x), xmm(x), [arg1+16*0]
  for i = 1, nrounds do
    | vaesenc xmm(x), xmm(x), [arg1+16*i]
  end
  | vaesenclast xmm(x), xmm(x), [arg1+16*(nrounds+1)]
end

local function prologue(Dst)
  for i = 12, 15 do
    | push Rq(i)
  end
  | mov r14, rsp
  | sub rsp, 16*8
  | and rsp, -64
end

local function epilogue(Dst)
  | mov rsp, r14
  for i = 15, 12, -1 do
    | pop Rq(i)
  end
  | ret
end

local function gcm_enc_dec(Dst, operation, pc, nrounds)
  prologue(Dst)

  | mov r13, arg4
  | and r13, -16
  | mov r12, r13
  | shr r12, 4
  | and r12, 7
  | jz =>pc+0
  for i = 7, 2, -1 do
    | cmp r12, i
    | je =>pc+i
  end
  | jmp =>pc+1
  for i = 7, 0, -1 do
    |=>pc+i:
    || initial_blocks(Dst, i, {12, 13, 14, 15}, 9, 0, operation, nrounds)
    if i ~= 0 then
      | sub r13, 16*i
      | jmp >8
    end
  end

  |8:
  | cmp r13, 0
  | je >1
  | sub r13, 128
  | je >2
  | vmovd r15d, xmm9
  | and r15d, 255
  | vpshufb xmm9, xmm9, [->shuf_mask]
  |3:
  | cmp r15b, 255-8
  | jg >4
  || ghash_8_encrypt_8_parallel(Dst, {0, 10, 11, 12, 13, 14, 15}, 9, "out_order", operation, nrounds)
  | jne <3
  | vpshufb xmm9, xmm9, [->shuf_mask]
  | jmp >2
  |4:
  | vpshufb xmm9, xmm9, [->shuf_mask]
  || ghash_8_encrypt_8_parallel(Dst, {0, 10, 11, 12, 13, 14, 15}, 9, "in_order", operation, nrounds)
  | vpshufb xmm9, xmm9, [->shuf_mask]
  | jne <3
  | vpshufb xmm9, xmm9, [->shuf_mask]
  |2:
  || ghash_last_8(Dst, {0, 10, 11, 12, 13, 14, 15})
  |1:

  | mov r13, arg4
  | and r13, 15
  | je >1

  | vpaddd xmm9, xmm9, [->one]
  | vpshufb xmm9, xmm9, [->shuf_mask]
  || encrypt_single_block(Dst, 9, nrounds)

  | sub r11, 16
  | add r11, r13
  | vmovdqu xmm1, [arg3 + r11]
  | lea r12, [->all_f]
  | sub r12, r13
  | vmovdqu xmm2, [r12]
  | vpshufb xmm1, xmm1, xmm2

  if operation == "dec" then
    | vmovdqa xmm2, xmm1
  end
  | vpxor xmm9, xmm9, xmm1
  | vmovdqu xmm1, [r12 + 16]
  | vpand xmm9, xmm9, xmm1
  if operation == "dec" then
    | vpand xmm2, xmm2, xmm1
  else
    | vmovdqa xmm2, xmm9
  end
  | vpshufb xmm2, xmm2, [->shuf_mask]
  | vpxor xmm14, xmm14, xmm2
  || ghash_mul(Dst, 14, 13, 0, 10, 11)
  | sub r11, r13
  | add r11, 16

  | vmovd rax, xmm9
  | cmp r13, 8
  | jle >2
  | mov [arg2 + r11], rax
  | add r11, 8
  | vpsrldq xmm9, xmm9, 8
  | vmovd rax, xmm9
  | sub r13, 8
  |2:
  | mov byte [arg2 + r11], al
  | add r11, 1
  | shr rax, 8
  | sub r13, 1
  | jne <2

  |1:
  | mov r12, arg7
  | shl r12, 3
  | vmovd xmm15, r12d

  | shl arg4, 3
  | vmovd xmm1, arg4
  | vpslldq xmm15, xmm15, 8
  | vpxor xmm15, xmm15, xmm1
        
  | vpxor xmm14, xmm14, xmm15
  || ghash_mul(Dst, 14, 13, 0, 10, 11)
  | vpshufb xmm14, xmm14, [->shuf_mask]
  | mov rax, arg5
  | vmovdqu xmm9, [rax]
  || encrypt_single_block(Dst, 9, nrounds)
  | vpxor xmm9, xmm9, xmm14

  | mov r10, arg8
  | mov r11, arg9
  | cmp r11, 16
  | je >3
  | cmp r11, 12
  | je >2
  | vmovd rax, xmm9
  | mov [r10], rax
  | jmp >4
  |2:
  | vmovd rax, xmm9
  | mov [r10], rax
  | vpsrldq xmm9, xmm9, 8
  | vmovd eax, xmm9
  | mov [r10 + 8], eax
  | jmp >4       
  |3:
  | vmovdqu [r10], xmm9
  |4:

  epilogue(Dst)
end

local function precompute(Dst)
  prologue(Dst)

  | vmovdqu xmm6, [arg2]
  | vpshufb xmm6, xmm6, [->shuf_mask]
  | vmovdqa xmm2, xmm6
  | vpsllq xmm6, xmm6, 1
  | vpsrlq xmm2, xmm2, 63
  | vmovdqa xmm1, xmm2
  | vpslldq xmm2, xmm2, 8 
  | vpsrldq xmm1, xmm1, 8
  | vpor xmm6, xmm6, xmm2     
  | vpshufd xmm2, xmm1, 0x24
  | vpcmpeqd xmm2, xmm2, [->two_one]
  | vpand xmm2, xmm2, [->poly]
  | vpxor xmm6, xmm6, xmm2
  | vmovdqa [arg1 + 16*15], xmm6

  | vmovdqa xmm4, xmm6
  for i = 2, 8 do
    || ghash_mul(Dst, 4, 6, 0, 1, 2)
    | vmovdqa [arg1 + 16*(14+i)], xmm4
  end

  epilogue(Dst)
end

local function keyexp_round (Dst, round)
  | vpshufd xmm2, xmm2, 0xff
  | vshufps xmm3, xmm3, xmm1, 0x10
  | vpxor xmm1, xmm1, xmm3
  | vshufps xmm3, xmm3, xmm1, 0x8c
  | vpxor xmm1, xmm1, xmm3
  | vpxor xmm1, xmm1, xmm2
end

local function keyexp2_round (Dst, round)
  | vpshufd xmm2, xmm2, 0xaa
  | vshufps xmm3, xmm3, xmm4, 0x10
  | vpxor xmm4, xmm4, xmm3
  | vshufps xmm3, xmm3, xmm4, 0x8c
  | vpxor xmm4, xmm4, xmm3
  | vpxor xmm4, xmm4, xmm2
end

local function keyexp128(Dst)
  | vmovdqu xmm1, [arg1]
  | vmovdqa [arg2], xmm1
  | vpxor xmm3, xmm3, xmm3
  for i = 1, 10 do
    | vaeskeygenassist xmm2, xmm1, i < 9 and 2^(i-1) or 27*(i-8)
    || keyexp_round(Dst)
    | vmovdqa [arg2 + 16*i], xmm1
  end
  | ret
end

local function keyexp256(Dst)
  | vmovdqu xmm1, [arg1]
  | vmovdqa [arg2], xmm1
  | vmovdqu xmm4, [arg1+16]
  | vmovdqa [arg2+16], xmm4
  | vpxor xmm3, xmm3, xmm3
  for i = 1, 6 do
    | vaeskeygenassist xmm2, xmm4, 2^(i-1)
    || keyexp_round(Dst)
    | vmovdqa [arg2 + 16*(i*2)], xmm1
    | vaeskeygenassist xmm2, xmm1, 2^(i-1)
    || keyexp2_round(Dst)
    | vmovdqa [arg2 + 16*(i*2+1)], xmm4
  end
  | vaeskeygenassist xmm2, xmm4, 0x40 -- 2^(7-1)
  || keyexp_round(Dst)
  | vmovdqa [arg2 + 16*14], xmm1
  | ret
end

local function aad_prehash(Dst)
   prologue(Dst)
   | vmovdqu xmm2, [arg1 + 16*15]
   | mov r10, arg3
   | mov r12, arg4

   | vpxor xmm0, xmm0, xmm0

   |1:
   | cmp r12, 16
   | jng >2
   | vmovdqu xmm1, [r10]
   | vpshufb xmm1, xmm1, [->shuf_mask]
   | vpxor xmm0, xmm0, xmm1
   ghash_mul(Dst, 0, 2, 13, 14, 15)
   | add r10, 16
   | sub r12, 16
   | jmp <1

   |2:
   | mov r11, r12
   | vpxor xmm1, xmm1, xmm1
   |1:
   | vmovd xmm3, dword [r10]
   | vpslldq xmm3, xmm3, 12
   | vpsrldq xmm1, xmm1, 4
   | vpxor xmm1, xmm1, xmm3
   | add r10, 4
   | sub r12, 4
   | jg <1
   | cmp r11, 16
   | je >3
   | mov r12, 16
   |2:
   | vpsrldq xmm1, xmm1, 4
   | sub r12, 4
   | cmp r12, r11
   | jg <2
   |3:
   | vpshufb xmm1, xmm1, [->shuf_mask]

   | vpxor xmm0, xmm0, xmm1

   | vpshufb xmm0, xmm0, [->shuf_mask] -- undone by initial_blocks
   | vmovdqu [arg2], xmm0
   epilogue(Dst)
end

local function auth16_equal(Dst)
   | mov rax, [arg1]
   | mov rdx, [arg1 + 8]
   | xor rax, [arg2]
   | xor rdx, [arg2 + 8]
   | or rax, rdx
   | ret
end

local function generator(Dst)
  Dst:growpc(32)

  -- Functions
  |->aesni_gcm_precomp_avx_gen4:
  || precompute(Dst)
  |.align 16
  |->aes_keyexp_128_enc_avx:
  || keyexp128(Dst)
  |.align 16
  |->aes_keyexp_256_enc_avx:
  || keyexp256(Dst)
  |.align 16
  |->aesni_gcm_enc_128_avx_gen4:
  || gcm_enc_dec(Dst, "enc", 0, 9)
  |.align 16
  |->aesni_gcm_enc_256_avx_gen4:
  || gcm_enc_dec(Dst, "enc", 8, 13)
  |.align 16
  |->aesni_gcm_dec_128_avx_gen4:
  || gcm_enc_dec(Dst, "dec", 16, 9)
  |.align 16
  |->aesni_gcm_dec_256_avx_gen4:
  || gcm_enc_dec(Dst, "dec", 24, 13)
  |.align 16
  |->aesni_encrypt_128_single_block:
  | vmovdqu xmm0, [arg2]
  || encrypt_single_block(Dst, 0, 9)
  | vmovdqu [arg2], xmm0
  | ret
  |.align 16
  |->aesni_encrypt_256_single_block:
  | vmovdqu xmm0, [arg2]
  || encrypt_single_block(Dst, 0, 13)
  | vmovdqu [arg2], xmm0
  | ret
  |.align 16
  |->auth16_equal:
  || auth16_equal(Dst)
  |.align 16
  |->aad_prehash:
  || aad_prehash(Dst)

  -- Data
  |.align 64
  |->poly:;    .dword          1, 0, 0, 0xC2000000
  |->poly2:;   .dword 0xC2000000, 1, 0, 0xC2000000
  |->two_one:; .dword          1, 0, 0,          1
  |->shuf_mask:
  for i = 15, 0, -1 do
    |.byte i
  end
  for i = 0, 15 do
    |.byte i
  end
  |->all_f:; .dword -1, -1, -1,   -1
  |          .dword  0,  0,  0,    0
  |->one:;   .dword  1,  0,  0,    0
  |->onef:;  .dword  0,  0,  0, 2^24
end

-- USAGE
--
-- First allocate and initialize a gcm_data state using the encryption key and
-- a derived hash subkey. This is done i.e. like so:
--
--   gcm_data = ffi.new("gcm_data __attribute__((aligned(16)))")
--   aes_keyexp_128_enc_avx(key, gcm_data)
--   (or aes_keyexp_256_enc_avx)
--   hash_subkey = ffi.new("uint8_t[16]")
--   aesni_encrypt_128_single_block(gcm_data, hash_subkey)
--   (or aesni_encrypt_256_single_block)
--   aesni_gcm_precomp_avx_gen4(gcm_data, hash_subkey)
-- 
-- Then your gcm_data state is ready for use with aesni_gcm_enc_*_avx_gen4,
-- aesni_gcm_dec_*_avx_gen4, and aad_prehash.
--
-- If your AAD is <= 16 bytes you can pad it to 16 bytes with zero bits and
-- pass it to aesni_gcm_enc_*_avx_gen4 and aesni_gcm_dec_*_avx_gen4 as is.
-- However, if your AAD exceeds 16 bytes you need to pre-hash it using
-- aad_prehash and use the resulting 16 byte output. You still need to pass the
-- actual length of the AAD (i.e., as supplied to aad_prehash) to
-- aesni_gcm_enc_*_avx_gen4 and aesni_gcm_dec_*_avx_gen4!

-- You can also use aad_prehash on <= 16 byte AAD, but that amounts to a NOOP.
-- Either way, the AAD’s length MUST be a multiple of four (i.e., not zero).
--
-- Make sure to check the resulting authentication tags, i.e. using
-- auth16_equal!
--
-- Arguments to aes_keyexp_128_enc:
--  1) uint8_t  key[16]
--  2) gcm_data *state       (aligned to 16 bytes)
--
-- Arguments to aes_keyexp_256_enc:
--  1) uint8_t  key[32]
--  2) gcm_data *state       (aligned to 16 bytes)
--
-- Arguments to aesni_encrypt_128_single_block, aesni_encrypt_256_single_block:
--  1) gcm_data *state       (aligned to 16 bytes)
--  2) uint8_t block[16]
--
-- Arguments to aesni_gcm_precomp_avx_gen4:
--  1) gcm_data *state       (aligned to 16 bytes)
--  2) uint8_t  hash_subkey[16]
--
-- Arguments to aad_prehash:
--  1) gcm_data *state      (aligned to 16 bytes)
--  2) uint8_t  out[16]
--  3) uint8_t  aad[aadlen] (AAD material)
--  4) uint64_t aadlen      (a multiple of 4)
--
-- Arguments to aesni_gcm_enc_128_avx_gen4, aesni_gcm_dec_128_avx_gen4,
-- aesni_gcm_enc_256_avx_gen4, aesni_gcm_dec_256_avx_gen4:
--  1) gcm_data *state      (aligned to 16 bytes)
--  2) uint8_t  out[len]
--  3) uint8_t  in[len]     (can be the same as out, i.e. "in-place")
--  4) uint64_t len
--  5) uint8_t  iv[16]      (96 bit IV, padded with bytes 0001, aligned to 16 bytes)
--  6) uint8_t  aad[16]     (16 bytes, output of aad_prehash if aadlen > 16)
--  7) uint64_t aadlen      (a multiple of 4, the actual aadlen as passed to aad_prehash)
--  8) uint8_t  tag[taglen]
--  9) uint64_t taglen      (should be 16 for all intents and purposes)
--
-- Arguments to auth16_equal:
--  1) uint8_t x[16]
--  2) uint8_t y[16]
-- Returns:
--  zero only if x and y are equal.

local Dst, globals = dasm.new(actions, nil, nil, 1 + #globalnames)
generator(Dst)
local mcode, size = Dst:build()
local entry = dasm.globals(globals, globalnames)
local fn_t = ffi.typeof("void(*)(gcm_data*, uint8_t*, const uint8_t*, uint64_t, const uint8_t*, const uint8_t*, uint64_t, uint8_t*, uint64_t)")
return setmetatable({
  aes_keyexp_128_enc_avx = ffi.cast("void(*)(const uint8_t*, gcm_data*)", entry.aes_keyexp_128_enc_avx),
  aes_keyexp_256_enc_avx = ffi.cast("void(*)(const uint8_t*, gcm_data*)", entry.aes_keyexp_256_enc_avx),
  aesni_gcm_precomp_avx_gen4 = ffi.cast("void(*)(gcm_data*, const uint8_t*)", entry.aesni_gcm_precomp_avx_gen4),
  aad_prehash = ffi.cast("void(*)(gcm_data*, uint8_t[16], const uint8_t*, uint64_t)", entry.aad_prehash),
  aesni_gcm_enc_128_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_enc_128_avx_gen4),
  aesni_gcm_enc_256_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_enc_256_avx_gen4),
  aesni_gcm_dec_128_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_dec_128_avx_gen4),
  aesni_gcm_dec_256_avx_gen4 = ffi.cast(fn_t, entry.aesni_gcm_dec_256_avx_gen4),
  aesni_encrypt_128_single_block = ffi.cast("void(*)(gcm_data*, uint8_t*)", entry.aesni_encrypt_128_single_block),
  aesni_encrypt_256_single_block = ffi.cast("void(*)(gcm_data*, uint8_t*)", entry.aesni_encrypt_256_single_block),
  auth16_equal = ffi.cast("uint64_t(*)(uint8_t[16], uint8_t[16])", entry.auth16_equal)
}, {_anchor = mcode})
